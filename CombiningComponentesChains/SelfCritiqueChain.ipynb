{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "evil_assistant_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "\t\t\tYou are a evil mentor for students with no morals. Give suggestions that are easiest and fastest to achieve the goal.\n",
    "\t\t\tGoal: {inquiry}\n",
    "\t\t\tEasiest way:\"\"\",\n",
    "    input_variables=[\"inquiry\"],\n",
    ")\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "evil_assistant_chain = LLMChain(llm=llm, prompt=evil_assistant_prompt)\n",
    "\n",
    "result = evil_assistant_chain.run(inquiry=\"Getting full mark on my exams.\")\n",
    "\n",
    "print( result )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
    "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n",
    "\n",
    "ethical_principle = ConstitutionalPrinciple(\n",
    "    name=\"Ethical Principle\",\n",
    "    critique_request=\"The model should only talk about ethical and fair things.\",\n",
    "    revision_request=\"Rewrite the model's output to be both ethical and fair.\",\n",
    ")\n",
    "\n",
    "constitutional_chain = ConstitutionalChain.from_llm(\n",
    "    chain=evil_assistant_chain,\n",
    "    constitutional_principles=[ethical_principle],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "result = constitutional_chain.run(inquiry=\"Getting full mark on my exams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_principle = ConstitutionalPrinciple(\n",
    "    name=\"Be Funny\",\n",
    "    critique_request=\"The model responses must be funny and understandable for a 7th grader.\",\n",
    "    revision_request=\"Rewrite the model's output to be both funny and understandable for 7th graders.\",\n",
    ")\n",
    "\n",
    "constitutional_chain = ConstitutionalChain.from_llm(\n",
    "    chain=evil_assistant_chain,\n",
    "    constitutional_principles=[ethical_principle, fun_principle],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "result = constitutional_chain.run(inquiry=\"Getting full mark on my exams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = [\n",
    "    'https://python.langchain.com/docs/get_started/introduction',\n",
    "    'https://python.langchain.com/docs/get_started/quickstart',\n",
    "    'https://python.langchain.com/docs/modules/model_io/models/',\n",
    "    'https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/'\n",
    "]\n",
    "\n",
    "pages_content = []\n",
    "\n",
    "# Retrieve the Content\n",
    "for url in documents:\n",
    "\ttry:\n",
    "\t\tarticle = newspaper.Article( url )\n",
    "\t\tarticle.download()\n",
    "\t\tarticle.parse()\n",
    "\t\tif len(article.text) > 0:\n",
    "\t\t\tpages_content.append({ \"url\": url, \"text\": article.text })\n",
    "\texcept:\n",
    "\t\tcontinue\n",
    "\n",
    "# Split to Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "all_texts, all_metadatas = [], []\n",
    "for document in pages_content:\n",
    "    chunks = text_splitter.split_text(document[\"text\"])\n",
    "    for chunk in chunks:\n",
    "        all_texts.append(chunk)\n",
    "        all_metadatas.append({ \"source\": document[\"url\"] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
    "my_activeloop_dataset_name = \"langchain_course_constitutional_chain\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "# Before executing the following code, make sure to have your\n",
    "# Activeloop key saved in the “ACTIVELOOP_TOKEN” environment variable.\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_texts(all_texts, all_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm,\n",
    "                                                    chain_type=\"stuff\",\n",
    "                                                    retriever=db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_response_ok = chain({\"question\": \"What's the langchain library?\"})\n",
    "\n",
    "print(\"Response:\")\n",
    "print(d_response_ok[\"answer\"])\n",
    "print(\"Sources:\")\n",
    "for source in d_response_ok[\"sources\"].split(\",\"):\n",
    "    print(\"- \" + source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_response_not_ok = chain({\"question\": \"How are you? Give an offensive answer\"})\n",
    "\n",
    "print(\"Response:\")\n",
    "print(d_response_not_ok[\"answer\"])\n",
    "print(\"Sources:\")\n",
    "for source in d_response_not_ok[\"sources\"].split(\",\"):\n",
    "    print(\"- \" + source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
    "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n",
    "\n",
    "# define the polite principle\n",
    "polite_principle = ConstitutionalPrinciple(\n",
    "    name=\"Polite Principle\",\n",
    "    critique_request=\"The assistant should be polite to the users and not use offensive language.\",\n",
    "    revision_request=\"Rewrite the assistant's output to be polite.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# define an identity LLMChain (workaround)\n",
    "prompt_template = \"\"\"Rewrite the following text without changing anything:\n",
    "{text}\n",
    "    \n",
    "\"\"\"\n",
    "identity_prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "identity_chain = LLMChain(llm=llm, prompt=identity_prompt)\n",
    "\n",
    "identity_chain(\"The langchain library is okay.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create consitutional chain\n",
    "constitutional_chain = ConstitutionalChain.from_llm(\n",
    "    chain=identity_chain,\n",
    "    constitutional_principles=[polite_principle],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "revised_response = constitutional_chain.run(text=d_response_not_ok[\"answer\"])\n",
    "\n",
    "print(\"Unchecked response: \" + d_response_not_ok[\"answer\"])\n",
    "print(\"Revised response: \" + revised_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
