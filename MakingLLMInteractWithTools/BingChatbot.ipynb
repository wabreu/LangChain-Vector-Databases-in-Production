{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "template = \"\"\"You are an assistant that answers the following question correctly and honestly: {question}\\n\\n\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "\n",
    "question_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "question_chain.run(\"what is the latest fast and furious movie?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"<Custom_Search_Engine_ID>\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"<Google_API_Key>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<OpenAI_Key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "TOP_N_RESULTS = 10\n",
    "\n",
    "def top_n_results(query):\n",
    "    return search.results(query, TOP_N_RESULTS)\n",
    "\n",
    "tool = Tool(\n",
    "    name = \"Google Search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=top_n_results\n",
    ")\n",
    "\n",
    "query = \"What is the latest fast and furious movie?\"\n",
    "\n",
    "results = tool.run(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result[\"title\"])\n",
    "    print(result[\"link\"])\n",
    "    print(result[\"snippet\"])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "\n",
    "pages_content = []\n",
    "\n",
    "for result in results:\n",
    "\ttry:\n",
    "\t\tarticle = newspaper.Article(result[\"link\"])\n",
    "\t\tarticle.download()\n",
    "\t\tarticle.parse()\n",
    "\t\tif len(article.text) > 0:\n",
    "\t\t\tpages_content.append({ \"url\": result[\"link\"], \"text\": article.text })\n",
    "\texcept:\n",
    "\t\tcontinue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=100)\n",
    "\n",
    "docs = []\n",
    "for d in pages_content:\n",
    "\tchunks = text_splitter.split_text(d[\"text\"])\n",
    "\tfor chunk in chunks:\n",
    "\t\tnew_doc = Document(page_content=chunk, metadata={ \"source\": d[\"url\"] })\n",
    "\t\tdocs.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "docs_embeddings = embeddings.embed_documents([doc.page_content for doc in docs])\n",
    "query_embedding = embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_top_k_indices(list_of_doc_vectors, query_vector, top_k):\n",
    "  # convert the lists of vectors to numpy arrays\n",
    "  list_of_doc_vectors = np.array(list_of_doc_vectors)\n",
    "  query_vector = np.array(query_vector)\n",
    "\n",
    "  # compute cosine similarities\n",
    "  similarities = cosine_similarity(query_vector.reshape(1, -1), list_of_doc_vectors).flatten()\n",
    "\n",
    "  # sort the vectors based on cosine similarity\n",
    "  sorted_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "  # retrieve the top K indices from the sorted list\n",
    "  top_k_indices = sorted_indices[:top_k]\n",
    "\n",
    "  return top_k_indices\n",
    "\n",
    "top_k = 2\n",
    "best_indexes = get_top_k_indices(docs_embeddings, query_embedding, top_k)\n",
    "best_k_documents = [doc for i, doc in enumerate(docs) if i in best_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "\n",
    "response = chain({\"input_documents\": best_k_documents, \"question\": query}, return_only_outputs=True)\n",
    "\n",
    "response_text, response_sources = response[\"output_text\"].split(\"SOURCES:\")\n",
    "response_text = response_text.strip()\n",
    "response_sources = response_sources.strip()\n",
    "\n",
    "print(f\"Answer: {response_text}\")\n",
    "print(f\"Sources: {response_sources}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
