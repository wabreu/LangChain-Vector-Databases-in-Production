{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "class DeepLakeLoader:\n",
    "    def __init__(self, source_data_path):\n",
    "    self.source_data_path = source_data_path\n",
    "    self.file_name = os.path.basename(source_data_path) # What we'll name our database \n",
    "    self.data = self.split_data()\n",
    "    if self.check_if_db_exists():\n",
    "        self.db = self.load_db()\n",
    "    else:\n",
    "        self.db = self.create_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(self):  \n",
    "    \"\"\"  \n",
    "    Preprocess the data by splitting it into passages.  \n",
    "\n",
    "    If using a different data source, this function will need to be modified.  \n",
    "\n",
    "    Returns:  \n",
    "        split_data (list): List of passages.  \n",
    "    \"\"\"  \n",
    "    with open(self.source_data_path, 'r') as f:  \n",
    "        content = f.read()  \n",
    "    split_data = re.split(r'(?=\\d+\\. )', content)\n",
    "    if split_data[0] == '':  \n",
    "        split_data.pop(0)  \n",
    "    split_data = [entry for entry in split_data if len(entry) >= 30]  \n",
    "    return split_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(self):  \n",
    "    \"\"\"  \n",
    "    Load the database if it already exists.  \n",
    "\n",
    "    Returns:  \n",
    "        DeepLake: DeepLake object.  \n",
    "    \"\"\"  \n",
    "    return DeepLake(dataset_path=f'deeplake/{self.file_name}', embedding_function=OpenAIEmbeddings(), read_only=True)  \n",
    "\n",
    "def create_db(self):  \n",
    "    \"\"\"  \n",
    "    Create the database if it does not already exist.  \n",
    "\n",
    "    Databases are stored in the deeplake directory.  \n",
    "\n",
    "    Returns:  \n",
    "        DeepLake: DeepLake object.  \n",
    "    \"\"\"  \n",
    "    return DeepLake.from_texts(self.data, OpenAIEmbeddings(), dataset_path=f'deeplake/{self.file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def query_db(self, query):  \n",
    "    \"\"\"  \n",
    "    Query the database for passages that are similar to the query.  \n",
    "\n",
    "    Args:  \n",
    "        query (str): Query string.  \n",
    "\n",
    "    Returns:  \n",
    "        content (list): List of passages that are similar to the query.  \n",
    "    \"\"\"  \n",
    "    results = self.db.similarity_search(query, k=3)  \n",
    "    content = []  \n",
    "    for result in results:  \n",
    "        content.append(result.page_content)  \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DeepLakeLoader('data/salestesting.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.query_db(detected_objection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "system_message = SystemMessage(content=objection_prompt)\n",
    "human_message = HumanMessage(content=f'Customer objection: {detected_objection} | Relevant guidelines: {results}')\n",
    "\n",
    "response = chat([system_message, human_message])\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
