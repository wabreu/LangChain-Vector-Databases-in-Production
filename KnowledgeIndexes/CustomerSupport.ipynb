{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://beebom.com/what-is-nft-explained/',\n",
    "        'https://beebom.com/how-delete-spotify-account/',\n",
    "        'https://beebom.com/how-download-gif-twitter/',\n",
    "        'https://beebom.com/how-use-chatgpt-linux-terminal/',\n",
    "        'https://beebom.com/how-delete-spotify-account/',\n",
    "        'https://beebom.com/how-save-instagram-story-with-music/',\n",
    "        'https://beebom.com/how-install-pip-windows/',\n",
    "        'https://beebom.com/how-check-disk-usage-linux/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the selenium scraper to load the documents\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "docs_not_splitted = loader.load()\n",
    "\n",
    "# we split the documents into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(docs_not_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
    "my_activeloop_dataset_name = \"langchain_course_customer_support\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the top relevant documents to a specific query\n",
    "query = \"how to check disk usage in linux?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer questions using information extracted from our db\n",
    "template = \"\"\"You are an exceptional customer support chatbot that gently answer questions.\n",
    "\n",
    "You know the following context information.\n",
    "\n",
    "{chunks_formatted}\n",
    "\n",
    "Answer to the following question from a customer. Use only information from the previous context information. Do not invent stuff.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chunks_formatted\", \"query\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full pipeline\n",
    "\n",
    "# user question\n",
    "query = \"How to check disk usage in linux?\"\n",
    "\n",
    "# retrieve relevant chunks\n",
    "docs = db.similarity_search(query)\n",
    "retrieved_chunks = [doc.page_content for doc in docs]\n",
    "\n",
    "# format the prompt\n",
    "chunks_formatted = \"\\n\\n\".join(retrieved_chunks)\n",
    "prompt_formatted = prompt.format(chunks_formatted=chunks_formatted, query=query)\n",
    "\n",
    "# generate answer\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "answer = llm(prompt_formatted)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
