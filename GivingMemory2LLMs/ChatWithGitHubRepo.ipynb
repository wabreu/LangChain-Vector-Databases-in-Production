{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "root_dir = \"./path/to/cloned/repository\"\n",
    "docs = []\n",
    "file_extensions = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "\n",
    "    for file in filenames:\n",
    "        file_path = os.path.join(dirpath, file)\n",
    "        if file_extensions and os.path.splitext(file)[1] not in file_extensions:\n",
    "            continue\n",
    "\t\n",
    "    loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    docs.extend(loader.load_and_split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splitted_text = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
    "my_activeloop_dataset_name = \"langchain_course_chat_with_gh\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(splitted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "# Create a retriever from the DeepLake instance\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Set the search parameters for the retriever\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "retriever.search_kwargs[\"k\"] = 10\n",
    "\n",
    "# Create a ChatOpenAI model instance\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a RetrievalQA instance from the model and retriever\n",
    "qa = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# Return the result of the query\n",
    "qa.run(\"What is the repository's name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install streamlit streamlit_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "\n",
    "# Set the title for the Streamlit app\n",
    "st.title(f\"Chat with GitHub Repository\")\n",
    "\n",
    "# Initialize the session state for placeholder messages.\n",
    "if \"generated\" not in st.session_state:\n",
    "\tst.session_state[\"generated\"] = [\"i am ready to help you ser\"]\n",
    "\n",
    "if \"past\" not in st.session_state:\n",
    "\tst.session_state[\"past\"] = [\"hello\"]\n",
    "\n",
    "# A field input to receive user queries\n",
    "user_input = st.text_input(\"\", key=\"input\")\n",
    "\n",
    "# Search the databse and add the responses to state\n",
    "if user_input:\n",
    "\toutput = qa.run(user_input)\n",
    "\tst.session_state.past.append(user_input)\n",
    "\tst.session_state.generated.append(output)\n",
    "\n",
    "# Create the conversational UI using the previous states\n",
    "if st.session_state[\"generated\"]:\n",
    "\tfor i in range(len(st.session_state[\"generated\"])):\n",
    "\t\tmessage(st.session_state[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
    "\t\tmessage(st.session_state[\"generated\"][i], key=str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## streamlit run ./chat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## git clone https://github.com/peterw/Chat-with-Git-Repo.git\n",
    "## cd Chat-with-Git-Repo\n",
    "\n",
    "## pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cp .env.example .env\n",
    "\n",
    "# OPENAI_API_KEY=your_openai_api_key\n",
    "# ACTIVELOOP_TOKEN=your_activeloop_api_token\n",
    "# ACTIVELOOP_USERNAME=your_activeloop_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python src/main.py process --repo-url https://github.com/username/repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
